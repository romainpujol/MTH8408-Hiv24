{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MTH8408 : Méthodes d'optimisation et contrôle optimal\n",
    " ## Laboratoire 3: Optimisation sans contraintes et méthodes itératives\n",
    "Tangi Migot et Paul Raynaud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, NLPModels, Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNLPModel - Model with automatic differentiation backend ADModelBackend{\n",
       "  ForwardDiffADGradient,\n",
       "  ForwardDiffADHvprod,\n",
       "  EmptyADbackend,\n",
       "  EmptyADbackend,\n",
       "  EmptyADbackend,\n",
       "  ForwardDiffADHessian,\n",
       "  EmptyADbackend,\n",
       "}\n",
       "  Problem name: Generic\n",
       "   All variables: ████████████████████ 2      All constraints: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "            free: ████████████████████ 2                 free: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "         low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "          infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "            nnzh: (  0.00% sparsity)   3               linear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "                                                    nonlinear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "                                                         nnzj: (------% sparsity)         \n",
       "\n",
       "  Counters:\n",
       "             obj: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 grad: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 cons: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "        cons_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0             cons_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 jcon: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           jgrad: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                  jac: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              jac_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "         jac_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                jprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0            jprod_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "       jprod_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               jtprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0           jtprod_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "      jtprod_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 hess: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                hprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           jhess: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               jhprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test problem:\n",
    "using ADNLPModels\n",
    "fH(x) = (x[2]+x[1].^2-11).^2+(x[1]+x[2].^2-7).^2\n",
    "x0H = [10., 20.]\n",
    "himmelblau = ADNLPModel(fH, x0H)\n",
    "\n",
    "problem2 = ADNLPModel(x->-x[1]^2, ones(3))\n",
    "\n",
    "roz(x) = 100 *  (x[2] - x[1]^2)^2 + (x[1] - 1.0)^2\n",
    "rosenbrock = ADNLPModel(roz, [-1.2, 1.0])\n",
    "\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "pb_du_cours = ADNLPModel(f, [-1.001, -1.001]) #ou [1.5, .5] ou [.5, .5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaires sur Julia\n",
    "\n",
    "Quelques commentaires sur des morceaux de codes que vous avez vu:\n",
    "- les structures, exemple [GenericExecutionStats](https://github.com/JuliaSmoothOptimizers/SolverCore.jl/blob/0091f437a26a27ac8aa53d5e37647223722f7f7c/src/stats.jl#L60) (constructeur, attribut, type).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SolverCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "Base.Meta.ParseError",
     "evalue": "ParseError:\n# Error @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n? GenericExecutionStats\n╙ ── not a unary operator",
     "output_type": "error",
     "traceback": [
      "ParseError:\n",
      "# Error @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n",
      "? GenericExecutionStats\n",
      "╙ ── not a unary operator\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? GenericExecutionStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Les arguments dans les fonctions. Lire attentivement [la documentation Julia sur les fonctions](https://docs.julialang.org/en/v1/manual/functions/) pour comprendre l'utilisation des `Optional Arguments` et des `Keywords Arguments`. Ce type d'arguments est très utile dans nos applictions où les solveurs dépendent de paramètre dont on peut fixer des valeurs par défaut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1: Méthode BFGS avec mémoire limitée (L-BFGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cet exercice est d'implémenter la méthode BFGS à mémoire limitée vue en cours en utilisant les `InverseLBFGSOperator` du package `LinearOperators.jl`. Il y a aussi un petit exemple dans la documentation du package [LinearOperators.jl/dev/tutorial/#Limited-memory-BFGS-and-SR1](https://juliasmoothoptimizers.github.io/LinearOperators.jl/dev/tutorial/#Limited-memory-BFGS-and-SR1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearOperators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "Base.Meta.ParseError",
     "evalue": "ParseError:\n# Error @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n? InverseLBFGSOperator\n╙ ── not a unary operator",
     "output_type": "error",
     "traceback": [
      "ParseError:\n",
      "# Error @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n",
      "? InverseLBFGSOperator\n",
      "╙ ── not a unary operator\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? InverseLBFGSOperator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ce qui est important dans ce type de méthode est:\n",
    "- le paramètre mémoire\n",
    "- la mise à jour de l'opérateur avec la fonction `push!`\n",
    "- si on a pas une direction de descente, alors on skip\n",
    "- recherche linéaire d'Armijo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "Base.Meta.ParseError",
     "evalue": "ParseError:\n# Error @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n? LinearOperators.push!\n╙ ── not a unary operator",
     "output_type": "error",
     "traceback": [
      "ParseError:\n",
      "# Error @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n",
      "? LinearOperators.push!\n",
      "╙ ── not a unary operator\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? LinearOperators.push!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "armijo (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function armijo(xk, dk, fk, gk, slope, nlp :: AbstractNLPModel; τ1 = 1.0e-4, t_update = 1.5)\n",
    "  t = 1.0\n",
    "  fk_new = obj(nlp, xk + dk) # t = 1.0\n",
    "  while fk_new > fk + τ1 * t * slope\n",
    "    t /= t_update\n",
    "    fk_new = obj(nlp, xk + t * dk)\n",
    "  end\n",
    "  return t, fk_new\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "limited_bfgs (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function limited_bfgs(nlp      :: AbstractNLPModel;\n",
    "                      x        :: AbstractVector = nlp.meta.x0,\n",
    "                      atol     :: Real = √eps(eltype(x)), \n",
    "                      rtol     :: Real = √eps(eltype(x)),\n",
    "                      max_eval :: Int = -1,\n",
    "                      max_time :: Float64 = 30.0,\n",
    "                      f_min    :: Float64 = -1.0e16,\n",
    "                      verbose  :: Bool = true,\n",
    "                      mem      :: Int = 5)\n",
    "  start_time = time()\n",
    "  elapsed_time = 0.0\n",
    "\n",
    "  T = eltype(x)\n",
    "  n = nlp.meta.nvar\n",
    "\n",
    "  xt = zeros(T, n)\n",
    "  ∇ft = zeros(T, n)\n",
    "\n",
    "  f = obj(nlp, x)\n",
    "  ∇f = grad(nlp, x)\n",
    "#################################################\n",
    "  H = ### Use InverseLBFGSOperator instead of I ###\n",
    "#################################################\n",
    "\n",
    "  ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "  ϵ = atol + rtol * ∇fNorm\n",
    "  iter = 0\n",
    "\n",
    "  @info log_header([:iter, :f, :dual, :slope, :bk], [Int, T, T, T, T],\n",
    "                   hdr_override=Dict(:f=>\"f(x)\", :dual=>\"‖∇f‖\", :slope=>\"∇fᵀd\"))\n",
    "\n",
    "  optimal = ∇fNorm ≤ ϵ\n",
    "  unbdd = f ≤ f_min\n",
    "  tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  stalled = false\n",
    "  status = :unknown\n",
    "\n",
    "  while !(optimal || tired || stalled || unbdd)\n",
    "\n",
    "#################################################\n",
    "    d = ### Compute d\n",
    "#################################################\n",
    "    slope = dot(d, ∇f)\n",
    "    if slope ≥ 0\n",
    "      @error \"not a descent direction\" slope\n",
    "      status = :not_desc\n",
    "      stalled = true\n",
    "      continue\n",
    "    end\n",
    "\n",
    "    # Perform improved Armijo linesearch.\n",
    "    t, ft = armijo(x, d, f, ∇f, slope, nlp)\n",
    "        \n",
    "    @info log_row(Any[iter, f, ∇fNorm, slope, t])\n",
    "\n",
    "    # Update L-BFGS approximation.\n",
    "    xt = x + t * d\n",
    "    ∇ft = grad(nlp, xt) # grad!(nlp, xt, ∇ft)\n",
    "#################################################\n",
    "    push! ### Update H\n",
    "#################################################\n",
    "\n",
    "    # Move on.\n",
    "    x = xt\n",
    "    f = ft\n",
    "    ∇f = ∇ft\n",
    "\n",
    "    ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "    iter = iter + 1\n",
    "\n",
    "    optimal = ∇fNorm ≤ ϵ\n",
    "    unbdd = f ≤ f_min\n",
    "    elapsed_time = time() - start_time\n",
    "    tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  end\n",
    "  @info log_row(Any[iter, f, ∇fNorm])\n",
    "\n",
    "  if optimal\n",
    "    status = :first_order\n",
    "  elseif tired\n",
    "    if neval_obj(nlp) > max_eval ≥ 0\n",
    "      status = :max_eval\n",
    "    elseif elapsed_time > max_time\n",
    "      status = :max_time\n",
    "    end\n",
    "  elseif unbdd\n",
    "        status = :unbounded\n",
    "  end\n",
    "\n",
    "  return GenericExecutionStats(\n",
    "        nlp,\n",
    "        status=status,\n",
    "        solution=x,\n",
    "        objective=f,\n",
    "        dual_feas=∇fNorm,\n",
    "        iter=iter,\n",
    "        elapsed_time=elapsed_time,\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m\u001b[1mError During Test\u001b[22m\u001b[39m at "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39m\u001b[1mc:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:4\u001b[22m\n",
      "  Got exception outside of a @test\n",
      "  UndefVarError: `d` not defined\n",
      "  Stacktrace:\n",
      "    [1] \u001b[0m\u001b[1mlimited_bfgs\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mnlp\u001b[39m::\u001b[0mADNLPModel\u001b[90m{Float64, Vector{Float64}, Vector{Int64}}\u001b[39m; \u001b[90mx\u001b[39m::\u001b[0mVector\u001b[90m{Float64}\u001b[39m, \u001b[90matol\u001b[39m::\u001b[0mFloat64, \u001b[90mrtol\u001b[39m::\u001b[0mFloat64, \u001b[90mmax_eval\u001b[39m::\u001b[0mInt64, \u001b[90mmax_time\u001b[39m::\u001b[0mFloat64, \u001b[90mf_min\u001b[39m::\u001b[0mFloat64, \u001b[90mverbose\u001b[39m::\u001b[0mBool, \u001b[90mmem\u001b[39m::\u001b[0mInt64\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[36mMain\u001b[39m \u001b[90mc:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\\u001b[39m\u001b[90m\u001b[4mLab3-notebook.ipynb:41\u001b[24m\u001b[39m\n",
      "    [2] \u001b[0m\u001b[1mlimited_bfgs\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mnlp\u001b[39m::\u001b[0mADNLPModel\u001b[90m{Float64, Vector{Float64}, Vector{Int64}}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[36mMain\u001b[39m \u001b[90mc:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\\u001b[39m\u001b[90m\u001b[4mLab3-notebook.ipynb:1\u001b[24m\u001b[39m\n",
      "    [3] \u001b[0m\u001b[1m(::var\"#19#25\")\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[36mMain\u001b[39m \u001b[90mc:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\\u001b[39m\u001b[90m\u001b[4mLab3-notebook.ipynb:8\u001b[24m\u001b[39m\n",
      "    [4] \u001b[0m\u001b[1mwith_logstate\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mf\u001b[39m::\u001b[0mFunction, \u001b[90mlogstate\u001b[39m::\u001b[0mAny\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[90mBase.CoreLogging\u001b[39m \u001b[90m.\\\u001b[39m\u001b[90m\u001b[4mlogging.jl:515\u001b[24m\u001b[39m\n",
      "    [5] \u001b[0m\u001b[1mwith_logger\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mf\u001b[39m::\u001b[0mFunction, \u001b[90mlogger\u001b[39m::\u001b[0mBase.CoreLogging.NullLogger\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[90mBase.CoreLogging\u001b[39m \u001b[90m.\\\u001b[39m\u001b[90m\u001b[4mlogging.jl:627\u001b[24m\u001b[39m\n",
      "    [6] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[90mc:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\\u001b[39m\u001b[90m\u001b[4mLab3-notebook.ipynb:7\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "    [7] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[90mC:\\Users\\romai\\AppData\\Local\\Programs\\Julia-1.10.0\\share\\julia\\stdlib\\v1.10\\Test\\src\\\u001b[39m\u001b[90m\u001b[4mTest.jl:1577\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "    [8] top-level scope\n",
      "  \u001b[90m    @\u001b[39m \u001b[90mc:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\\u001b[39m\u001b[90m\u001b[4mLab3-notebook.ipynb:6\u001b[24m\u001b[39m\n",
      "    [9] \u001b[0m\u001b[1meval\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[90m.\\\u001b[39m\u001b[90m\u001b[4mboot.jl:385\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "   [10] \u001b[0m\u001b[1minclude_string\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mmapexpr\u001b[39m::\u001b[0mtypeof(REPL.softscope), \u001b[90mmod\u001b[39m::\u001b[0mModule, \u001b[90mcode\u001b[39m::\u001b[0mString, \u001b[90mfilename\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m.\\\u001b[39m\u001b[90m\u001b[4mloading.jl:2070\u001b[24m\u001b[39m\n",
      "   [11] \u001b[0m\u001b[1m#invokelatest#2\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[90m.\\\u001b[39m\u001b[90m\u001b[4messentials.jl:887\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "   [12] \u001b[0m\u001b[1minvokelatest\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[90m.\\\u001b[39m\u001b[90m\u001b[4messentials.jl:884\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "   [13] \u001b[0m\u001b[1m(::VSCodeServer.var\"#208#209\"{VSCodeServer.NotebookRunCellArguments, String})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[35mVSCodeServer\u001b[39m \u001b[90mc:\\Users\\romai\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\packages\\VSCodeServer\\src\\\u001b[39m\u001b[90m\u001b[4mserve_notebook.jl:19\u001b[24m\u001b[39m\n",
      "   [14] \u001b[0m\u001b[1mwithpath\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mf\u001b[39m::\u001b[0mVSCodeServer.var\"#208#209\"\u001b[90m{VSCodeServer.NotebookRunCellArguments, String}\u001b[39m, \u001b[90mpath\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[35mVSCodeServer\u001b[39m \u001b[90mc:\\Users\\romai\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\packages\\VSCodeServer\\src\\\u001b[39m\u001b[90m\u001b[4mrepl.jl:274\u001b[24m\u001b[39m\n",
      "   [15] \u001b[0m\u001b[1mnotebook_runcell_request\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mconn\u001b[39m::\u001b[0mVSCodeServer.JSONRPC.JSONRPCEndpoint\u001b[90m{Base.PipeEndpoint, Base.PipeEndpoint}\u001b[39m, \u001b[90mparams\u001b[39m::\u001b[0mVSCodeServer.NotebookRunCellArguments\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[35mVSCodeServer\u001b[39m \u001b[90mc:\\Users\\romai\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\packages\\VSCodeServer\\src\\\u001b[39m\u001b[90m\u001b[4mserve_notebook.jl:13\u001b[24m\u001b[39m\n",
      "   [16] \u001b[0m\u001b[1mdispatch_msg\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mx\u001b[39m::\u001b[0mVSCodeServer.JSONRPC.JSONRPCEndpoint\u001b[90m{Base.PipeEndpoint, Base.PipeEndpoint}\u001b[39m, \u001b[90mdispatcher\u001b[39m::\u001b[0mVSCodeServer.JSONRPC.MsgDispatcher, \u001b[90mmsg\u001b[39m::\u001b[0mDict\u001b[90m{String, Any}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[35mVSCodeServer.JSONRPC\u001b[39m \u001b[90mc:\\Users\\romai\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\packages\\JSONRPC\\src\\\u001b[39m\u001b[90m\u001b[4mtyped.jl:67\u001b[24m\u001b[39m\n",
      "   [17] \u001b[0m\u001b[1mserve_notebook\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mpipename\u001b[39m::\u001b[0mString, \u001b[90moutputchannel_logger\u001b[39m::\u001b[0mBase.CoreLogging.SimpleLogger; \u001b[90mcrashreporting_pipename\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[35mVSCodeServer\u001b[39m \u001b[90mc:\\Users\\romai\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\packages\\VSCodeServer\\src\\\u001b[39m\u001b[90m\u001b[4mserve_notebook.jl:139\u001b[24m\u001b[39m\n",
      "   [18] top-level scope\n",
      "  \u001b[90m    @\u001b[39m \u001b[90mc:\\Users\\romai\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\notebook\\\u001b[39m\u001b[90m\u001b[4mnotebook.jl:32\u001b[24m\u001b[39m\n",
      "   [19] \u001b[0m\u001b[1minclude\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mmod\u001b[39m::\u001b[0mModule, \u001b[90m_path\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m.\\\u001b[39m\u001b[90m\u001b[4mBase.jl:495\u001b[24m\u001b[39m\n",
      "   [20] \u001b[0m\u001b[1mexec_options\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mopts\u001b[39m::\u001b[0mBase.JLOptions\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m.\\\u001b[39m\u001b[90m\u001b[4mclient.jl:318\u001b[24m\u001b[39m\n",
      "   [21] \u001b[0m\u001b[1m_start\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m.\\\u001b[39m\u001b[90m\u001b[4mclient.jl:552\u001b[24m\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mTest Summary: | \u001b[22m\u001b[91m\u001b[1mError  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "test set      | \u001b[91m    1  \u001b[39m\u001b[36m    1  \u001b[39m\u001b[0m2.6s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TestSetException",
     "evalue": "Some tests did not pass: 0 passed, 0 failed, 1 errored, 0 broken.",
     "output_type": "error",
     "traceback": [
      "Some tests did not pass: 0 passed, 0 failed, 1 errored, 0 broken.\n",
      "\n",
      "Stacktrace:\n",
      " [1] finish(ts::Test.DefaultTestSet; print_results::Bool)\n",
      "   @ Test C:\\Users\\romai\\AppData\\Local\\Programs\\Julia-1.10.0\\share\\julia\\stdlib\\v1.10\\Test\\src\\Test.jl:1195\n",
      " [2] finish(ts::Test.DefaultTestSet)\n",
      "   @ Test C:\\Users\\romai\\AppData\\Local\\Programs\\Julia-1.10.0\\share\\julia\\stdlib\\v1.10\\Test\\src\\Test.jl:1170\n",
      " [3] macro expansion\n",
      "   @ C:\\Users\\romai\\AppData\\Local\\Programs\\Julia-1.10.0\\share\\julia\\stdlib\\v1.10\\Test\\src\\Test.jl:1593 [inlined]\n",
      " [4] top-level scope\n",
      "   @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:6"
     ]
    }
   ],
   "source": [
    "#Unit/Validation Tests\n",
    "# Réaliser un test unitaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "\n",
    "- Compare l'implémentation de `limited_bfgs` avec la fonction `lbfgs` qui est disponible dans `JSOSolvers.jl`.\n",
    "- On veut pouvoir tester \"facilement\" plusieurs valeurs de $\\tau$ et du paramètre de mise à jour dans `armijo` sur les problèmes tests. Comment modifier le code pour que ça soit possible?\n",
    "\n",
    "On peut mesurer deux executions de fonctions Julia grâce aux fonctions de `BenchmarkTools.jl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "Base.Meta.ParseError",
     "evalue": "ParseError:\n# Error @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n? @time\n╙ ── not a unary operator",
     "output_type": "error",
     "traceback": [
      "ParseError:\n",
      "# Error @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n",
      "? @time\n",
      "╙ ── not a unary operator\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? @time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: NewtonCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cet exercice est d'adapter les méthodes de Newton de façon à résoudre le système linéaire avec une méthode itérative de type gradient conjugué comme suit ($B_k$ représente la matrice hessienne):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LineSearchNewtonCG.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cg_optim (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function cg_optim(H, ∇f)\n",
    "    #setup the tolerance:\n",
    "    n∇f = norm(∇f)\n",
    "#####################################\n",
    "    ϵk = min(0.5)\n",
    "####################################\n",
    "    n = length(∇f)\n",
    "    z = zeros(n)\n",
    "    r = ∇f\n",
    "    d = -r\n",
    "    \n",
    "    j = 0\n",
    "    while norm(r) ≥ ϵk && j < 3 * n\n",
    "###############################################\n",
    "        if dot(d, H * d) ≤ 0\n",
    "            # TODO\n",
    "        end\n",
    "##############################################\n",
    "        α = # TODO\n",
    "##############################################        \n",
    "        z += α * d\n",
    "        nrr2 = dot(r, r)\n",
    "        r += α * H * d\n",
    "##############################################\n",
    "        β  = # TODO\n",
    "##############################################\n",
    "        d  = -r + β * d\n",
    "        j += 1\n",
    "    end\n",
    "    return z\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui est important ici est qu'on a pas besoin de stocker/évaluer la matrice hessienne entière mais simplement le produit entre la hessienne et un vecteur. Pour un `NLPModels` on utilise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "Base.Meta.ParseError",
     "evalue": "ParseError:\n# Error @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n? NLPModels.hprod\n╙ ── not a unary operator",
     "output_type": "error",
     "traceback": [
      "ParseError:\n",
      "# Error @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n",
      "? NLPModels.hprod\n",
      "╙ ── not a unary operator\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? NLPModels.hprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "Base.Meta.ParseError",
     "evalue": "ParseError:\n# Error @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n? NLPModels.hess_op\n╙ ── not a unary operator",
     "output_type": "error",
     "traceback": [
      "ParseError:\n",
      "# Error @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n",
      "? NLPModels.hess_op\n",
      "╙ ── not a unary operator\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? NLPModels.hess_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "armijo_Newton_cg (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function armijo_Newton_cg(nlp      :: AbstractNLPModel;\n",
    "                          x        :: AbstractVector = nlp.meta.x0,\n",
    "                          atol     :: Real = √eps(eltype(x)), \n",
    "                          rtol     :: Real = √eps(eltype(x)),\n",
    "                          max_eval :: Int = -1,\n",
    "                          max_time :: Float64 = 30.0,\n",
    "                          f_min    :: Float64 = -1.0e16)\n",
    "  start_time = time()\n",
    "  elapsed_time = 0.0\n",
    "\n",
    "  T = eltype(x)\n",
    "  n = nlp.meta.nvar\n",
    "\n",
    "  f = obj(nlp, x)\n",
    "  ∇f = grad(nlp, x)\n",
    "#################################################\n",
    "  H = # Initialize H as linear operator representing the Hessian matrix\n",
    "#################################################\n",
    "\n",
    "  ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "  ϵ = atol + rtol * ∇fNorm\n",
    "  iter = 0\n",
    "\n",
    "  @info log_header([:iter, :f, :dual, :slope, :bk], [Int, T, T, T, T],\n",
    "                   hdr_override=Dict(:f=>\"f(x)\", :dual=>\"‖∇f‖\", :slope=>\"∇fᵀd\"))\n",
    "\n",
    "  optimal = ∇fNorm ≤ ϵ\n",
    "  unbdd = f ≤ f_min\n",
    "  tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  stalled = false\n",
    "  status = :unknown\n",
    "\n",
    "  while !(optimal || tired || stalled || unbdd)\n",
    "        \n",
    "    d = cg_optim(H, ∇f)\n",
    "        \n",
    "    slope = dot(d, ∇f)\n",
    "    if slope ≥ 0\n",
    "      @error \"not a descent direction\" slope\n",
    "      status = :not_desc\n",
    "      stalled = true\n",
    "      continue\n",
    "    end\n",
    "\n",
    "    # Perform improved Armijo linesearch.\n",
    "    t, f = armijo(x, d, f, ∇f, slope, nlp)\n",
    "        \n",
    "    @info log_row(Any[iter, f, ∇fNorm, slope, t])\n",
    "\n",
    "    # Update L-BFGS approximation.\n",
    "    x += t * d\n",
    "    ∇f = grad(nlp, x)\n",
    "#################################################\n",
    "    H = ### Update H\n",
    "#################################################\n",
    "\n",
    "    ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "    iter = iter + 1\n",
    "\n",
    "    optimal = ∇fNorm ≤ ϵ\n",
    "    unbdd = f ≤ f_min\n",
    "    elapsed_time = time() - start_time\n",
    "    tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  end\n",
    "  @info log_row(Any[iter, f, ∇fNorm])\n",
    "\n",
    "  if optimal\n",
    "    status = :first_order\n",
    "  elseif tired\n",
    "    if neval_obj(nlp) > max_eval ≥ 0\n",
    "      status = :max_eval\n",
    "    elseif elapsed_time > max_time\n",
    "      status = :max_time\n",
    "    end\n",
    "  elseif unbdd\n",
    "        status = :unbounded\n",
    "  end\n",
    "\n",
    "  return GenericExecutionStats(nlp, status = status, solution=x, objective=f, dual_feas=∇fNorm,\n",
    "                               iter=iter, elapsed_time=elapsed_time)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unit/Validation Tests\n",
    "# Réaliser un test unitaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment préparer un benchmark\n",
    "\n",
    "On veut maintenant pouvoir réaliser un benchmark de plusieurs solveurs. Pour comparer les algorithmes, il nous faut une collection de problèmes tests et on va utiliser `OptimizationProblems.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OptimizationProblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez trouver un tutoriel de comment réaliser un benchmark avec ce package sur la documentation [OptimizationProblems.jl/dev/benchmark/](https://juliasmoothoptimizers.github.io/OptimizationProblems.jl/dev/benchmark/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est fort possible que les petits problèmes tests que l'on résout après l'implémentation ne suffisent pas à déceler des bugs. Mais on peut toujours analyser l'éxecution de notre algorithme sur certains problèmes de la collection afin d'améliorer la valeur de certains paramètres (limite de temps, d'itérations, d'évaluations), détecter un bug, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Package NLPModelsJuMP not found in current path.\n- Run `import Pkg; Pkg.add(\"NLPModelsJuMP\")` to install the NLPModelsJuMP package.",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package NLPModelsJuMP not found in current path.\n",
      "- Run `import Pkg; Pkg.add(\"NLPModelsJuMP\")` to install the NLPModelsJuMP package.\n",
      "\n",
      "Stacktrace:\n",
      "  [1] macro expansion\n",
      "    @ Base .\\loading.jl:1766 [inlined]\n",
      "  [2] macro expansion\n",
      "    @ Base .\\lock.jl:267 [inlined]\n",
      "  [3] __require(into::Module, mod::Symbol)\n",
      "    @ Base .\\loading.jl:1747\n",
      "  [4] #invoke_in_world#3\n",
      "    @ Base .\\essentials.jl:921 [inlined]\n",
      "  [5] invoke_in_world\n",
      "    @ Base .\\essentials.jl:918 [inlined]\n",
      "  [6] require(into::Module, mod::Symbol)\n",
      "    @ Base .\\loading.jl:1740\n",
      "  [7] eval\n",
      "    @ .\\boot.jl:385 [inlined]\n",
      "  [8] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base .\\loading.jl:2070\n",
      "  [9] #invokelatest#2\n",
      "    @ .\\essentials.jl:887 [inlined]\n",
      " [10] invokelatest\n",
      "    @ .\\essentials.jl:884 [inlined]\n",
      " [11] (::VSCodeServer.var\"#208#209\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer c:\\Users\\romai\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:19\n",
      " [12] withpath(f::VSCodeServer.var\"#208#209\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer c:\\Users\\romai\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\packages\\VSCodeServer\\src\\repl.jl:274\n",
      " [13] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer c:\\Users\\romai\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:13\n",
      " [14] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC c:\\Users\\romai\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\packages\\JSONRPC\\src\\typed.jl:67\n",
      " [15] serve_notebook(pipename::String, outputchannel_logger::Base.CoreLogging.SimpleLogger; crashreporting_pipename::String)\n",
      "    @ VSCodeServer c:\\Users\\romai\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:139\n",
      " [16] top-level scope\n",
      "    @ c:\\Users\\romai\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\notebook\\notebook.jl:32"
     ]
    }
   ],
   "source": [
    "using OptimizationProblems.PureJuMP, NLPModelsJuMP\n",
    "jump_model = AMPGO02() # OptimizationProblems.PureJuMP.AMPGO02\n",
    "prbl = MathOptNLPModel(jump_model)\n",
    "limited_bfgs(prbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous vous en doutez pour le rapport de cette semaine on va vouloir réaliser une benchmark avec les deux méthodes que l'on a codé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix:\n",
    "\n",
    "Une petite remarque sur la gestion de la mémoire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 1\n",
      "b = 1\n",
      "b = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Pour les nombres:\n",
    "a = 1\n",
    "@show a\n",
    "b = a\n",
    "@show b\n",
    "a = 2\n",
    "@show b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [0.0, 0.0]\n",
      "(a, b) = ([0.0, 0.0], [0.0, 0.0])\n",
      "(a, b) = ([1.0, 1.0], [0.0, 0.0])\n",
      "(a, b) = ([1.0, 1.0], [1.0, 1.0])\n",
      "(a, b) = ([2.0, 2.0], [2.0, 2.0])\n",
      "(a, b) = ([1.0, 1.0], [1.0, 1.0])\n",
      "(a, b) = ([2.0, 2.0], [1.0, 1.0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.0, 2.0], [1.0, 1.0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Pour les tableaux:\n",
    "a = zeros(Float64, 2) #or zeros(2)\n",
    "@show a\n",
    "b = a\n",
    "@show (a,b)\n",
    "a = ones(Float64, 2)\n",
    "@show (a,b)\n",
    "\n",
    "#Pour les tableaux:\n",
    "a = ones(Float64, 2)\n",
    "b = a\n",
    "@show (a,b)\n",
    "a .= 2*ones(Float64, 2) #same would go with grad!\n",
    "@show (a,b)\n",
    "\n",
    "#Pour les tableaux:\n",
    "a = ones(Float64, 2)\n",
    "b = copy(a) # or similar(a)\n",
    "@show (a,b)\n",
    "a .= 2 .* ones(Float64, 2) #same would go with grad!\n",
    "@show (a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `nlp` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `nlp` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:2"
     ]
    }
   ],
   "source": [
    "#Pour les NLPModels, il existe aussi des fonctions qui interviennent sur la mémoire\n",
    "gk = grad(nlp, x0)\n",
    "grad!(nlp, x0, gk) #équivaut à gk .= grad(nlp, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparez les `grad` et `grad!` à l'aide de `@benchmark` ainsi que:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: `@benchmark` not defined\nin expression starting at c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:5",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: `@benchmark` not defined\n",
      "in expression starting at c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:5\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "a = rand(n)\n",
    "b = ones(n)\n",
    "c = similar(a)\n",
    "@benchmark c = 2 * a + b * 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: `@benchmark` not defined\nin expression starting at c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: `@benchmark` not defined\n",
      "in expression starting at c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1\n"
     ]
    }
   ],
   "source": [
    "@benchmark c .= 2 .* a .+ b .* 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En particulier, observez la mémoire allouée et le temps moyen requis pour performer la ligne d'instruction.\n",
    "La mémoire nécessaire varie en fonction du nombre et du type d'opération effectué.\n",
    "Parmis les opérations allouant le moins de mémoire, on retrouve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: `@benchmark` not defined\nin expression starting at c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: `@benchmark` not defined\n",
      "in expression starting at c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1\n"
     ]
    }
   ],
   "source": [
    "@benchmark c .= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour manipuler plus finement la mémoire au cours de votre implémentation, vous pouvez utilisez des fonctions de `LinearAlgebra`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "Base.Meta.ParseError",
     "evalue": "ParseError:\n# Error @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n? axpy!\n╙ ── not a unary operator",
     "output_type": "error",
     "traceback": [
      "ParseError:\n",
      "# Error @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n",
      "? axpy!\n",
      "╙ ── not a unary operator\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? axpy!\n",
    "? axpby!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qui sont des routines `BLAS`.\n",
    "Faites attention, ces routines ont des effets de bord sur les structures de données `y`.\n",
    "Dès lors, l'utilisation de `@benchmark` accumule successivement les effets de bord, d'où l'utilisation des tests avant `@benchmark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y == a * x = true\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: `@benchmark` not defined\nin expression starting at c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:7",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: `@benchmark` not defined\n",
      "in expression starting at c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:7\n"
     ]
    }
   ],
   "source": [
    "y = zeros(n)\n",
    "x = ones(n)\n",
    "a = 2\n",
    "axpy!(a, x, y)\n",
    "@show y == a * x\n",
    "\n",
    "@benchmark axpy!(a, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y == a * ones(n) + b * π * ones(n) = true\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: `@benchmark` not defined\nin expression starting at c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:8",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: `@benchmark` not defined\n",
      "in expression starting at c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:8\n"
     ]
    }
   ],
   "source": [
    "y = π * ones(n)\n",
    "x = ones(n)\n",
    "a = 2\n",
    "b = 3\n",
    "axpby!(a, x, b, y)\n",
    "@show y == a * ones(n) + b * π * ones(n)\n",
    "\n",
    "@benchmark axpby!(a, x, b, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela fontionne aussi pour les matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y == a * ones(n, n) + b * π * ones(n, n) = true\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: `@benchmark` not defined\nin expression starting at c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:8",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: `@benchmark` not defined\n",
      "in expression starting at c:\\Users\\romai\\POLYMTL\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:8\n"
     ]
    }
   ],
   "source": [
    "y = π * ones(n,n)\n",
    "x = ones(n,n)\n",
    "a = 2\n",
    "b = 3\n",
    "axpby!(a, x, b, y)\n",
    "@show y == a * ones(n,n) + b * π * ones(n,n)\n",
    "\n",
    "@benchmark axpby!(a, x, b, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
